{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ –Ω–µ —É–ª—É—á—à–∏–ª–æ –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—é –≤—Å–µ–π –∫–æ–ª–æ–Ω–∫–∏ –æ–±—ä–µ–º–æ–≤\n",
    "# import sqlite3\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # === 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===\n",
    "# db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\RTS_futures_options_day.db')\n",
    "\n",
    "# with sqlite3.connect(db_path) as conn:\n",
    "#     df_fut = pd.read_sql_query(\n",
    "#         \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Futures\",\n",
    "#         conn\n",
    "#     )\n",
    "\n",
    "# # === 2. –§–£–ù–ö–¶–ò–Ø –ö–û–î–ò–†–û–í–ê–ù–ò–Ø –°–í–ï–ß–ï–ô (–õ–ò–•–û–í–ò–î–û–í) –° –£–ß–ï–¢–û–ú –û–ë–™–ï–ú–ê ===\n",
    "# low_quantile = df_fut['VOLUME'].quantile(0.33)\n",
    "# high_quantile = df_fut['VOLUME'].quantile(0.66)\n",
    "\n",
    "# def encode_volume(volume):\n",
    "#     if volume <= low_quantile:\n",
    "#         return '0'  # –ù–∏–∑–∫–∏–π –æ–±—ä–µ–º\n",
    "#     elif volume <= high_quantile:\n",
    "#         return '1'  # –°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º\n",
    "#     else:\n",
    "#         return '2'  # –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º\n",
    "\n",
    "# def encode_candle(row):\n",
    "#     open_, low, high, close, volume = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE'], row['VOLUME']\n",
    "\n",
    "#     if close > open_:\n",
    "#         direction = 1  \n",
    "#     elif close < open_:\n",
    "#         direction = 0  \n",
    "#     else:\n",
    "#         direction = 2  \n",
    "\n",
    "#     upper_shadow = high - max(open_, close)\n",
    "#     lower_shadow = min(open_, close) - low\n",
    "#     body = abs(close - open_)\n",
    "\n",
    "#     def classify_shadow(shadow, body):\n",
    "#         if shadow < 0.1 * body:\n",
    "#             return 0  \n",
    "#         elif shadow < 0.5 * body:\n",
    "#             return 1  \n",
    "#         else:\n",
    "#             return 2  \n",
    "\n",
    "#     upper_code = classify_shadow(upper_shadow, body)\n",
    "#     lower_code = classify_shadow(lower_shadow, body)\n",
    "\n",
    "#     volume_code = encode_volume(volume)  # –ö–æ–¥ –æ–±—ä–µ–º–∞\n",
    "\n",
    "#     return f\"{direction}{upper_code}{lower_code}{volume_code}\"  # –ö–æ–¥ —Å–≤–µ—á–∏ + –æ–±—ä–µ–º\n",
    "\n",
    "# df_fut['CANDLE_CODE'] = df_fut.apply(encode_candle, axis=1)\n",
    "\n",
    "# # === 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===\n",
    "# unique_codes = sorted(df_fut['CANDLE_CODE'].unique())  # –£—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –∫–æ–¥—ã —Å –æ–±—ä–µ–º–æ–º\n",
    "# code_to_int = {code: i for i, code in enumerate(unique_codes)}\n",
    "# df_fut['CANDLE_INT'] = df_fut['CANDLE_CODE'].map(code_to_int)\n",
    "\n",
    "# window_size = 20  \n",
    "# predict_offset = 1  \n",
    "\n",
    "# X, y = [], []\n",
    "# for i in range(len(df_fut) - window_size - predict_offset):\n",
    "#     X.append(df_fut['CANDLE_INT'].iloc[i:i+window_size].values)\n",
    "    \n",
    "#     # –ù–æ–≤—ã–π —Ç–∞—Ä–≥–µ—Ç: 1 ‚Äî —Ä–æ—Å—Ç, 0 ‚Äî –ø–∞–¥–µ–Ω–∏–µ\n",
    "#     y.append(1 if df_fut['CLOSE'].iloc[i+window_size+predict_offset] > df_fut['CLOSE'].iloc[i+window_size] else 0)\n",
    "\n",
    "# X, y = np.array(X), np.array(y)\n",
    "\n",
    "# split = int(0.8 * len(X))\n",
    "# X_train, y_train = X[:split], y[:split]\n",
    "# X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# class CandlestickDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.tensor(X, dtype=torch.long)\n",
    "#         self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "# train_dataset = CandlestickDataset(X_train, y_train)\n",
    "# test_dataset = CandlestickDataset(X_test, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # === 4. –°–û–ó–î–ê–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (LSTM) ===\n",
    "# class CandleLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "#         super(CandleLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.fc(x[:, -1, :])  \n",
    "#         return self.sigmoid(x)\n",
    "\n",
    "# # === 5. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –õ–£–ß–®–ï–ô ===\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CandleLSTM(vocab_size=len(unique_codes), embedding_dim=8, hidden_dim=32, output_dim=1).to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# best_accuracy = 0  \n",
    "# model_path = \"best_model_05.pth\"\n",
    "\n",
    "# epochs = 2000\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch).squeeze()\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ ===\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in test_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             y_pred = model(X_batch).squeeze().round()\n",
    "#             correct += (y_pred == y_batch).sum().item()\n",
    "#             total += y_batch.size(0)\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "#     # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ===\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "#         print(f\"‚úÖ Model saved with accuracy: {best_accuracy:.2%}\")\n",
    "\n",
    "# # === 6. –ó–ê–ì–†–£–ó–ö–ê –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ò –¢–ï–°–¢ ===\n",
    "# print(\"\\nüîπ Loading best model for final evaluation...\")\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         y_pred = model(X_batch).squeeze().round()\n",
    "#         correct += (y_pred == y_batch).sum().item()\n",
    "#         total += y_batch.size(0)\n",
    "\n",
    "# final_accuracy = correct / total\n",
    "# print(f\"üèÜ Final Test Accuracy: {final_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ (2 –ö–õ–ê–°–°–ê: –£–ú–ï–ù–¨–®–ò–õ–°–Ø / –£–í–ï–õ–ò–ß–ò–õ–°–Ø –ò–õ–ò –ù–ï –ò–ó–ú–ï–ù–ò–õ–°–Ø)\n",
    "# import sqlite3\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # === 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===\n",
    "# db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\RTS_futures_options_day.db')\n",
    "\n",
    "# with sqlite3.connect(db_path) as conn:\n",
    "#     df_fut = pd.read_sql_query(\n",
    "#         \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Futures\",\n",
    "#         conn\n",
    "#     )\n",
    "\n",
    "# # === 2. –§–£–ù–ö–¶–ò–Ø –ö–û–î–ò–†–û–í–ê–ù–ò–Ø –°–í–ï–ß–ï–ô (–õ–ò–•–û–í–ò–î–û–í) ===\n",
    "# def encode_candle(row):\n",
    "#     open_, low, high, close = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE']\n",
    "\n",
    "#     if close > open_:\n",
    "#         direction = 1  \n",
    "#     elif close < open_:\n",
    "#         direction = 0  \n",
    "#     else:\n",
    "#         direction = 2  \n",
    "\n",
    "#     upper_shadow = high - max(open_, close)\n",
    "#     lower_shadow = min(open_, close) - low\n",
    "#     body = abs(close - open_)\n",
    "\n",
    "#     def classify_shadow(shadow, body):\n",
    "#         if shadow < 0.1 * body:\n",
    "#             return 0  \n",
    "#         elif shadow < 0.5 * body:\n",
    "#             return 1  \n",
    "#         else:\n",
    "#             return 2  \n",
    "\n",
    "#     upper_code = classify_shadow(upper_shadow, body)\n",
    "#     lower_code = classify_shadow(lower_shadow, body)\n",
    "\n",
    "#     return f\"{direction}{upper_code}{lower_code}\"\n",
    "\n",
    "# df_fut['CANDLE_CODE'] = df_fut.apply(encode_candle, axis=1)\n",
    "\n",
    "# # === 3. –ö–û–î–ò–†–û–í–ê–ù–ò–ï –û–ë–™–ï–ú–û–í (2 –ö–õ–ê–°–°–ê: –£–ú–ï–ù–¨–®–ò–õ–°–Ø / –£–í–ï–õ–ò–ß–ò–õ–°–Ø –ò–õ–ò –ù–ï –ò–ó–ú–ï–ù–ò–õ–°–Ø) ===\n",
    "# df_fut['VOLUME_CHANGE'] = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é (–¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏)\n",
    "# df_fut.loc[1:, 'VOLUME_CHANGE'] = np.where(\n",
    "#     df_fut['VOLUME'].iloc[1:].values < df_fut['VOLUME'].shift(1).iloc[1:].values, 0, 1\n",
    "# )\n",
    "\n",
    "# # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–¥ —Å–≤–µ—á–∏ –∏ –∫–æ–¥ –æ–±—ä–µ–º–∞ –≤ –æ–¥–∏–Ω –∫–æ–¥\n",
    "# df_fut['COMBINED_CODE'] = df_fut['CANDLE_CODE'] + df_fut['VOLUME_CHANGE'].astype(str)\n",
    "\n",
    "# # === 4. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ù–ï–ô–†–û–°–ï–¢–ò ===\n",
    "# unique_codes = sorted(df_fut['COMBINED_CODE'].unique())\n",
    "# code_to_int = {code: i for i, code in enumerate(unique_codes)}\n",
    "# df_fut['CANDLE_INT'] = df_fut['COMBINED_CODE'].map(code_to_int)\n",
    "\n",
    "# window_size = 20  \n",
    "# predict_offset = 1  \n",
    "\n",
    "# X, y = [], []\n",
    "# for i in range(len(df_fut) - window_size - predict_offset):\n",
    "#     X.append(df_fut['CANDLE_INT'].iloc[i:i+window_size].values)\n",
    "#     y.append(1 if df_fut['CLOSE'].iloc[i+window_size+predict_offset] > df_fut['CLOSE'].iloc[i+window_size] else 0)\n",
    "\n",
    "# X, y = np.array(X), np.array(y)\n",
    "\n",
    "# split = int(0.8 * len(X))\n",
    "# X_train, y_train = X[:split], y[:split]\n",
    "# X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# class CandlestickDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.tensor(X, dtype=torch.long)\n",
    "#         self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "# train_dataset = CandlestickDataset(X_train, y_train)\n",
    "# test_dataset = CandlestickDataset(X_test, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # === 5. –°–û–ó–î–ê–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (LSTM) ===\n",
    "# class CandleLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "#         super(CandleLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.fc(x[:, -1, :])  \n",
    "#         return self.sigmoid(x)\n",
    "\n",
    "# # === 6. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –õ–£–ß–®–ï–ô ===\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CandleLSTM(vocab_size=len(unique_codes), embedding_dim=8, hidden_dim=32, output_dim=1).to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# best_accuracy = 0  \n",
    "# model_path = \"best_model_05.pth\"\n",
    "\n",
    "# epochs = 2000\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch).squeeze()\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ ===\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in test_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             y_pred = model(X_batch).squeeze().round()\n",
    "#             correct += (y_pred == y_batch).sum().item()\n",
    "#             total += y_batch.size(0)\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "#     # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ===\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "#         print(f\"‚úÖ Model saved with accuracy: {best_accuracy:.2%}\")\n",
    "\n",
    "# # === 7. –ó–ê–ì–†–£–ó–ö–ê –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ò –¢–ï–°–¢ ===\n",
    "# print(\"\\nüîπ Loading best model for final evaluation...\")\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         y_pred = model(X_batch).squeeze().round()\n",
    "#         correct += (y_pred == y_batch).sum().item()\n",
    "#         total += y_batch.size(0)\n",
    "\n",
    "# final_accuracy = correct / total\n",
    "# print(f\"üèÜ Final Test Accuracy: {final_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Loss: 0.6920, Test Accuracy: 50.50%, Best accuracy: 0.00%, Epoch best accuracy: 0, seed: 90\n",
      "‚úÖ Model saved with accuracy: 50.50%\n",
      "Epoch 2/2000, Loss: 0.6910, Test Accuracy: 50.50%, Best accuracy: 50.50%, Epoch best accuracy: 1, seed: 90\n",
      "Epoch 3/2000, Loss: 0.6909, Test Accuracy: 50.89%, Best accuracy: 50.50%, Epoch best accuracy: 1, seed: 90\n",
      "‚úÖ Model saved with accuracy: 50.89%\n",
      "Epoch 4/2000, Loss: 0.6897, Test Accuracy: 51.88%, Best accuracy: 50.89%, Epoch best accuracy: 3, seed: 90\n",
      "‚úÖ Model saved with accuracy: 51.88%\n",
      "Epoch 5/2000, Loss: 0.6886, Test Accuracy: 51.29%, Best accuracy: 51.88%, Epoch best accuracy: 4, seed: 90\n",
      "Epoch 6/2000, Loss: 0.6874, Test Accuracy: 51.29%, Best accuracy: 51.88%, Epoch best accuracy: 4, seed: 90\n",
      "Epoch 7/2000, Loss: 0.6855, Test Accuracy: 49.70%, Best accuracy: 51.88%, Epoch best accuracy: 4, seed: 90\n",
      "Epoch 8/2000, Loss: 0.6829, Test Accuracy: 51.29%, Best accuracy: 51.88%, Epoch best accuracy: 4, seed: 90\n",
      "Epoch 9/2000, Loss: 0.6801, Test Accuracy: 55.64%, Best accuracy: 51.88%, Epoch best accuracy: 4, seed: 90\n",
      "‚úÖ Model saved with accuracy: 55.64%\n",
      "Epoch 10/2000, Loss: 0.6777, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 11/2000, Loss: 0.6753, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 12/2000, Loss: 0.6702, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 13/2000, Loss: 0.6662, Test Accuracy: 53.47%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 14/2000, Loss: 0.6612, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 15/2000, Loss: 0.6563, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 16/2000, Loss: 0.6514, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 17/2000, Loss: 0.6472, Test Accuracy: 50.69%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 18/2000, Loss: 0.6423, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 19/2000, Loss: 0.6360, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 20/2000, Loss: 0.6337, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 21/2000, Loss: 0.6248, Test Accuracy: 48.91%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 22/2000, Loss: 0.6201, Test Accuracy: 48.32%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 23/2000, Loss: 0.6135, Test Accuracy: 49.31%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 24/2000, Loss: 0.6094, Test Accuracy: 49.31%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 25/2000, Loss: 0.6069, Test Accuracy: 48.12%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 26/2000, Loss: 0.5990, Test Accuracy: 48.91%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 27/2000, Loss: 0.5919, Test Accuracy: 49.70%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 28/2000, Loss: 0.5856, Test Accuracy: 49.90%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 29/2000, Loss: 0.5779, Test Accuracy: 49.31%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 30/2000, Loss: 0.5691, Test Accuracy: 48.71%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 31/2000, Loss: 0.5709, Test Accuracy: 50.10%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 32/2000, Loss: 0.5602, Test Accuracy: 48.51%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 33/2000, Loss: 0.5569, Test Accuracy: 49.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 34/2000, Loss: 0.5503, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 35/2000, Loss: 0.5409, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 36/2000, Loss: 0.5322, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 37/2000, Loss: 0.5274, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 38/2000, Loss: 0.5245, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 39/2000, Loss: 0.5161, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 40/2000, Loss: 0.5136, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 41/2000, Loss: 0.4987, Test Accuracy: 50.69%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 42/2000, Loss: 0.4916, Test Accuracy: 49.31%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 43/2000, Loss: 0.4934, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 44/2000, Loss: 0.4823, Test Accuracy: 48.71%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 45/2000, Loss: 0.4761, Test Accuracy: 50.10%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 46/2000, Loss: 0.4656, Test Accuracy: 49.11%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 47/2000, Loss: 0.4611, Test Accuracy: 49.90%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 48/2000, Loss: 0.4519, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 49/2000, Loss: 0.4453, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 50/2000, Loss: 0.4336, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 51/2000, Loss: 0.4346, Test Accuracy: 50.69%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 52/2000, Loss: 0.4300, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 53/2000, Loss: 0.4145, Test Accuracy: 49.31%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 54/2000, Loss: 0.4067, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 55/2000, Loss: 0.4119, Test Accuracy: 50.30%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 56/2000, Loss: 0.4042, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 57/2000, Loss: 0.3881, Test Accuracy: 50.10%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 58/2000, Loss: 0.3881, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 59/2000, Loss: 0.3755, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 60/2000, Loss: 0.3664, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 61/2000, Loss: 0.3542, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 62/2000, Loss: 0.3451, Test Accuracy: 49.90%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 63/2000, Loss: 0.3498, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 64/2000, Loss: 0.3508, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 65/2000, Loss: 0.3279, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 66/2000, Loss: 0.3198, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 67/2000, Loss: 0.3177, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 68/2000, Loss: 0.3061, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 69/2000, Loss: 0.2964, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 70/2000, Loss: 0.2972, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 71/2000, Loss: 0.2912, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 72/2000, Loss: 0.2809, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 73/2000, Loss: 0.2860, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 74/2000, Loss: 0.2676, Test Accuracy: 53.47%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 75/2000, Loss: 0.2967, Test Accuracy: 53.86%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 76/2000, Loss: 0.2808, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 77/2000, Loss: 0.2472, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 78/2000, Loss: 0.2391, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 79/2000, Loss: 0.2360, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 80/2000, Loss: 0.2312, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 81/2000, Loss: 0.2252, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 82/2000, Loss: 0.2259, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 83/2000, Loss: 0.2216, Test Accuracy: 53.66%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 84/2000, Loss: 0.2193, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 85/2000, Loss: 0.2109, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 86/2000, Loss: 0.2042, Test Accuracy: 54.26%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 87/2000, Loss: 0.2498, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 88/2000, Loss: 0.2367, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 89/2000, Loss: 0.2206, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 90/2000, Loss: 0.1908, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 91/2000, Loss: 0.2081, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 92/2000, Loss: 0.1907, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 93/2000, Loss: 0.1639, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 94/2000, Loss: 0.1571, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 95/2000, Loss: 0.1474, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 96/2000, Loss: 0.1447, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 97/2000, Loss: 0.1433, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 98/2000, Loss: 0.1362, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 99/2000, Loss: 0.1337, Test Accuracy: 53.47%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 100/2000, Loss: 0.1329, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 101/2000, Loss: 0.1240, Test Accuracy: 53.47%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 102/2000, Loss: 0.1211, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 103/2000, Loss: 0.1228, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 104/2000, Loss: 0.1441, Test Accuracy: 53.86%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 105/2000, Loss: 0.1300, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 106/2000, Loss: 0.1311, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 107/2000, Loss: 0.1087, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 108/2000, Loss: 0.1323, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 109/2000, Loss: 0.1494, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 110/2000, Loss: 0.1736, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 111/2000, Loss: 0.1318, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 112/2000, Loss: 0.1081, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 113/2000, Loss: 0.1140, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 114/2000, Loss: 0.0905, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 115/2000, Loss: 0.0810, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 116/2000, Loss: 0.0778, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 117/2000, Loss: 0.0732, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 118/2000, Loss: 0.0702, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 119/2000, Loss: 0.0678, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 120/2000, Loss: 0.0656, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 121/2000, Loss: 0.0703, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 122/2000, Loss: 0.0858, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 123/2000, Loss: 0.1328, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 124/2000, Loss: 0.1556, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 125/2000, Loss: 0.1078, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 126/2000, Loss: 0.1230, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 127/2000, Loss: 0.0936, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 128/2000, Loss: 0.0747, Test Accuracy: 50.50%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 129/2000, Loss: 0.0614, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 130/2000, Loss: 0.0534, Test Accuracy: 50.69%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 131/2000, Loss: 0.0491, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 132/2000, Loss: 0.0464, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 133/2000, Loss: 0.0446, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 134/2000, Loss: 0.0419, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 135/2000, Loss: 0.0402, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 136/2000, Loss: 0.0390, Test Accuracy: 50.69%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 137/2000, Loss: 0.0375, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 138/2000, Loss: 0.0360, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 139/2000, Loss: 0.0342, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 140/2000, Loss: 0.0341, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 141/2000, Loss: 0.0348, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 142/2000, Loss: 0.0337, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 143/2000, Loss: 0.0310, Test Accuracy: 51.09%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 144/2000, Loss: 0.1394, Test Accuracy: 50.10%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 145/2000, Loss: 0.2679, Test Accuracy: 48.91%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 146/2000, Loss: 0.2164, Test Accuracy: 51.29%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 147/2000, Loss: 0.1316, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 148/2000, Loss: 0.0929, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 149/2000, Loss: 0.0628, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 150/2000, Loss: 0.0516, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 151/2000, Loss: 0.0393, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 152/2000, Loss: 0.0382, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 153/2000, Loss: 0.0311, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 154/2000, Loss: 0.0289, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 155/2000, Loss: 0.0269, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 156/2000, Loss: 0.0255, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 157/2000, Loss: 0.0242, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 158/2000, Loss: 0.0230, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 159/2000, Loss: 0.0218, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 160/2000, Loss: 0.0208, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 161/2000, Loss: 0.0202, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 162/2000, Loss: 0.0192, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 163/2000, Loss: 0.0185, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 164/2000, Loss: 0.0269, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 165/2000, Loss: 0.1017, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 166/2000, Loss: 0.1584, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 167/2000, Loss: 0.1434, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 168/2000, Loss: 0.1538, Test Accuracy: 50.89%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 169/2000, Loss: 0.0663, Test Accuracy: 50.30%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 170/2000, Loss: 0.0573, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 171/2000, Loss: 0.0453, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 172/2000, Loss: 0.0336, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 173/2000, Loss: 0.0292, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 174/2000, Loss: 0.0270, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 175/2000, Loss: 0.0253, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 176/2000, Loss: 0.0190, Test Accuracy: 51.49%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 177/2000, Loss: 0.0167, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 178/2000, Loss: 0.0156, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 179/2000, Loss: 0.0147, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 180/2000, Loss: 0.0141, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 181/2000, Loss: 0.0134, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 182/2000, Loss: 0.0129, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 183/2000, Loss: 0.0124, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 184/2000, Loss: 0.0119, Test Accuracy: 51.68%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 185/2000, Loss: 0.0114, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 186/2000, Loss: 0.0110, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 187/2000, Loss: 0.0105, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 188/2000, Loss: 0.0101, Test Accuracy: 52.08%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 189/2000, Loss: 0.0098, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 190/2000, Loss: 0.0094, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 191/2000, Loss: 0.0091, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 192/2000, Loss: 0.0087, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 193/2000, Loss: 0.0084, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 194/2000, Loss: 0.0081, Test Accuracy: 51.88%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 195/2000, Loss: 0.0078, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 196/2000, Loss: 0.0075, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 197/2000, Loss: 0.0072, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 198/2000, Loss: 0.0069, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 199/2000, Loss: 0.0068, Test Accuracy: 53.07%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 200/2000, Loss: 0.0065, Test Accuracy: 52.28%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 201/2000, Loss: 0.0063, Test Accuracy: 53.66%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 202/2000, Loss: 0.0061, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 203/2000, Loss: 0.0058, Test Accuracy: 53.47%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 204/2000, Loss: 0.0056, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 205/2000, Loss: 0.0054, Test Accuracy: 52.87%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 206/2000, Loss: 0.0052, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 207/2000, Loss: 0.0050, Test Accuracy: 53.27%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 208/2000, Loss: 0.0049, Test Accuracy: 52.48%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "Epoch 209/2000, Loss: 0.0047, Test Accuracy: 52.67%, Best accuracy: 55.64%, Epoch best accuracy: 9, seed: 90\n",
      "üõë Early stopping at epoch 209\n",
      "\n",
      "üîπ Loading best model for final evaluation...\n",
      "üèÜ Final Test Accuracy: 55.64%\n"
     ]
    }
   ],
   "source": [
    "# –ö–≤–∞–Ω—Ç–∏–ª—å –æ–±—ä–µ–º–∞ —Ç–µ–ø–µ—Ä—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ 10 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–≤–µ—á–µ–π, –∞ –Ω–µ –Ω–∞ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "import sqlite3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === 1. –§–ò–ö–°–ê–¶–ò–Ø –°–õ–£–ß–ê–ô–ù–´–• –ß–ò–°–ï–õ –î–õ–Ø –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–û–°–¢–ò ===\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# 39, 50, 85, 90\n",
    "set_seed(90)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π seed\n",
    "\n",
    "# === 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===\n",
    "db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\RTS_futures_options_day.db')\n",
    "\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    df_fut = pd.read_sql_query(\n",
    "        \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Futures\",\n",
    "        conn\n",
    "    )\n",
    "\n",
    "# === 2. –§–£–ù–ö–¶–ò–Ø –ö–û–î–ò–†–û–í–ê–ù–ò–Ø –°–í–ï–ß–ï–ô (–õ–ò–•–û–í–ò–î–û–í) –° –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ú –ö–û–î–ò–†–û–í–ê–ù–ò–ï–ú –û–ë–™–ï–ú–ê ===\n",
    "window_size_volume = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–≤–∞–Ω—Ç–∏–ª–µ–π\n",
    "\n",
    "def encode_volume(index, volume):\n",
    "    if index < window_size_volume:  \n",
    "        return '1'  # –î–ª—è –ø–µ—Ä–≤—ã—Ö 10 —Å–≤–µ—á–µ–π —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º\n",
    "\n",
    "    past_volumes = df_fut['VOLUME'].iloc[index - window_size_volume:index]  \n",
    "    low_quantile = past_volumes.quantile(0.33)\n",
    "    high_quantile = past_volumes.quantile(0.66)\n",
    "\n",
    "    if volume <= low_quantile:\n",
    "        return '0'  # –ù–∏–∑–∫–∏–π –æ–±—ä–µ–º\n",
    "    elif volume <= high_quantile:\n",
    "        return '1'  # –°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º\n",
    "    else:\n",
    "        return '2'  # –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º\n",
    "\n",
    "def encode_candle(index, row):\n",
    "    open_, low, high, close, volume = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE'], row['VOLUME']\n",
    "\n",
    "    if close > open_:\n",
    "        direction = 1  \n",
    "    elif close < open_:\n",
    "        direction = 0  \n",
    "    else:\n",
    "        direction = 2  \n",
    "\n",
    "    upper_shadow = high - max(open_, close)\n",
    "    lower_shadow = min(open_, close) - low\n",
    "    body = abs(close - open_)\n",
    "\n",
    "    def classify_shadow(shadow, body):\n",
    "        if shadow < 0.1 * body:\n",
    "            return 0  \n",
    "        elif shadow < 0.5 * body:\n",
    "            return 1  \n",
    "        else:\n",
    "            return 2  \n",
    "\n",
    "    upper_code = classify_shadow(upper_shadow, body)\n",
    "    lower_code = classify_shadow(lower_shadow, body)\n",
    "\n",
    "    volume_code = encode_volume(index, volume)  # –ö–æ–¥ –æ–±—ä–µ–º–∞\n",
    "\n",
    "    return f\"{direction}{upper_code}{lower_code}{volume_code}\"  # –ö–æ–¥ —Å–≤–µ—á–∏ + –æ–±—ä–µ–º\n",
    "\n",
    "df_fut['CANDLE_CODE'] = [encode_candle(i, row) for i, row in df_fut.iterrows()]\n",
    "\n",
    "# === 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===\n",
    "unique_codes = sorted(df_fut['CANDLE_CODE'].unique())  \n",
    "code_to_int = {code: i for i, code in enumerate(unique_codes)}\n",
    "df_fut['CANDLE_INT'] = df_fut['CANDLE_CODE'].map(code_to_int)\n",
    "\n",
    "window_size = 20  \n",
    "predict_offset = 1  \n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df_fut) - window_size - predict_offset):\n",
    "    # Add features\n",
    "    X.append(df_fut['CANDLE_INT'].iloc[i:i+window_size].values)\n",
    "    \n",
    "    # –ù–æ–≤—ã–π —Ç–∞—Ä–≥–µ—Ç: 1 ‚Äî —Ä–æ—Å—Ç, 0 ‚Äî –ø–∞–¥–µ–Ω–∏–µ\n",
    "    y.append(\n",
    "        1 if df_fut['CLOSE'].iloc[i+window_size+predict_offset] > \n",
    "        df_fut['CLOSE'].iloc[i+window_size] else 0\n",
    "        )\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "class CandlestickDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(42 + worker_id)\n",
    "    random.seed(42 + worker_id)\n",
    "\n",
    "train_dataset = CandlestickDataset(X_train, y_train)\n",
    "test_dataset = CandlestickDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "# === 4. –°–û–ó–î–ê–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (LSTM) ===\n",
    "class CandleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(CandleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# === 5. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –õ–£–ß–®–ï–ô ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CandleLSTM(\n",
    "    vocab_size=len(unique_codes), embedding_dim=8, hidden_dim=32, output_dim=1\n",
    "    ).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_accuracy = 0  \n",
    "epoch_best_accuracy = 0\n",
    "model_path = \"bm_fut_lih_vol_02.pth\"\n",
    "early_stop_epochs = 200\n",
    "epochs_no_improve = 0\n",
    "\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch).squeeze().round()\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}, \"\n",
    "        f\"Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "        f\"Test Accuracy: {accuracy:.2%}, \"\n",
    "        f\"Best accuracy: {best_accuracy:.2%}, \"\n",
    "        f\"Epoch best accuracy: {epoch_best_accuracy}, \"\n",
    "        f\"seed: 90\"\n",
    "        )\n",
    "\n",
    "    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ===\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_no_improve = 0\n",
    "        epoch_best_accuracy = epoch + 1\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"‚úÖ Model saved with accuracy: {best_accuracy:.2%}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # === –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ ===\n",
    "    if epochs_no_improve >= early_stop_epochs:\n",
    "        print(f\"üõë Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# === 6. –ó–ê–ì–†–£–ó–ö–ê –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ò –¢–ï–°–¢ ===\n",
    "print(\"\\nüîπ Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch).squeeze().round()\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "final_accuracy = correct / total\n",
    "print(f\"üèÜ Final Test Accuracy: {final_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: üìâ DOWN\n",
      "UP Probability: 48.32%, \n",
      "DOWN Probability: 51.68%\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model.load_state_dict(torch.load(\"bm_fut_lih_vol_02.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å–≤–µ—á–µ–π –∏–∑ df_fut\n",
    "last_sequence = torch.tensor(\n",
    "    df_fut['CANDLE_INT'].iloc[-20:].values, dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "with torch.no_grad():\n",
    "    probability_up = model(last_sequence).item()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞\n",
    "    probability_down = 1 - probability_up  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞–¥–µ–Ω–∏—è\n",
    "\n",
    "    direction = \"üìà UP\" if probability_up >= 0.5 else \"üìâ DOWN\"\n",
    "\n",
    "    print(f\"Prediction: {direction}\")\n",
    "    print(\n",
    "        f\"UP Probability: {probability_up:.2%}, \\n\"\n",
    "        f\"DOWN Probability: {probability_down:.2%}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRADEDATE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>LOW</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>CANDLE_CODE</th>\n",
       "      <th>CANDLE_INT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>76930.0</td>\n",
       "      <td>72470.0</td>\n",
       "      <td>78980.0</td>\n",
       "      <td>74600.0</td>\n",
       "      <td>372848</td>\n",
       "      <td>0221</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>74470.0</td>\n",
       "      <td>71200.0</td>\n",
       "      <td>74610.0</td>\n",
       "      <td>73480.0</td>\n",
       "      <td>319307</td>\n",
       "      <td>0121</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>73490.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>81380.0</td>\n",
       "      <td>79980.0</td>\n",
       "      <td>537469</td>\n",
       "      <td>1111</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>79950.0</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>81050.0</td>\n",
       "      <td>77650.0</td>\n",
       "      <td>592715</td>\n",
       "      <td>0121</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>77210.0</td>\n",
       "      <td>73180.0</td>\n",
       "      <td>77550.0</td>\n",
       "      <td>73900.0</td>\n",
       "      <td>440908</td>\n",
       "      <td>0111</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>98600.0</td>\n",
       "      <td>98120.0</td>\n",
       "      <td>101480.0</td>\n",
       "      <td>100700.0</td>\n",
       "      <td>124069</td>\n",
       "      <td>1112</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>100570.0</td>\n",
       "      <td>100080.0</td>\n",
       "      <td>111010.0</td>\n",
       "      <td>109930.0</td>\n",
       "      <td>247083</td>\n",
       "      <td>1102</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>109750.0</td>\n",
       "      <td>106470.0</td>\n",
       "      <td>114130.0</td>\n",
       "      <td>108620.0</td>\n",
       "      <td>282242</td>\n",
       "      <td>0222</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>108960.0</td>\n",
       "      <td>108050.0</td>\n",
       "      <td>115370.0</td>\n",
       "      <td>115170.0</td>\n",
       "      <td>183356</td>\n",
       "      <td>1012</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>115100.0</td>\n",
       "      <td>112020.0</td>\n",
       "      <td>116500.0</td>\n",
       "      <td>112250.0</td>\n",
       "      <td>246559</td>\n",
       "      <td>0102</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2542 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TRADEDATE      OPEN       LOW      HIGH     CLOSE  VOLUME CANDLE_CODE  \\\n",
       "0     2015-01-05   76930.0   72470.0   78980.0   74600.0  372848        0221   \n",
       "1     2015-01-06   74470.0   71200.0   74610.0   73480.0  319307        0121   \n",
       "2     2015-01-08   73490.0   71000.0   81380.0   79980.0  537469        1111   \n",
       "3     2015-01-09   79950.0   74450.0   81050.0   77650.0  592715        0121   \n",
       "4     2015-01-12   77210.0   73180.0   77550.0   73900.0  440908        0111   \n",
       "...          ...       ...       ...       ...       ...     ...         ...   \n",
       "2537  2025-02-12   98600.0   98120.0  101480.0  100700.0  124069        1112   \n",
       "2538  2025-02-13  100570.0  100080.0  111010.0  109930.0  247083        1102   \n",
       "2539  2025-02-14  109750.0  106470.0  114130.0  108620.0  282242        0222   \n",
       "2540  2025-02-17  108960.0  108050.0  115370.0  115170.0  183356        1012   \n",
       "2541  2025-02-18  115100.0  112020.0  116500.0  112250.0  246559        0102   \n",
       "\n",
       "      CANDLE_INT  \n",
       "0             25  \n",
       "1             16  \n",
       "2             40  \n",
       "3             16  \n",
       "4             13  \n",
       "...          ...  \n",
       "2537          41  \n",
       "2538          38  \n",
       "2539          26  \n",
       "2540          32  \n",
       "2541          11  \n",
       "\n",
       "[2542 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
