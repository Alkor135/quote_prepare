{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÐŸÐ»Ð¾Ñ…Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alkor\\VSCode\\quote_prepare\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Loss: 0.6926, Test Accuracy: 50.50%\n",
      "âœ… Model saved with accuracy: 50.50%\n",
      "Epoch 2/2000, Loss: 0.6921, Test Accuracy: 49.90%\n",
      "Epoch 3/2000, Loss: 0.6914, Test Accuracy: 51.49%\n",
      "âœ… Model saved with accuracy: 51.49%\n",
      "Epoch 4/2000, Loss: 0.6906, Test Accuracy: 50.69%\n",
      "Epoch 5/2000, Loss: 0.6846, Test Accuracy: 51.49%\n",
      "Epoch 6/2000, Loss: 0.6794, Test Accuracy: 49.50%\n",
      "Epoch 7/2000, Loss: 0.6707, Test Accuracy: 50.69%\n",
      "Epoch 8/2000, Loss: 0.6691, Test Accuracy: 50.69%\n",
      "Epoch 9/2000, Loss: 0.6650, Test Accuracy: 51.68%\n",
      "âœ… Model saved with accuracy: 51.68%\n",
      "Epoch 10/2000, Loss: 0.6602, Test Accuracy: 50.89%\n",
      "Epoch 11/2000, Loss: 0.6491, Test Accuracy: 50.69%\n",
      "Epoch 12/2000, Loss: 0.6243, Test Accuracy: 52.28%\n",
      "âœ… Model saved with accuracy: 52.28%\n",
      "Epoch 13/2000, Loss: 0.5998, Test Accuracy: 53.86%\n",
      "âœ… Model saved with accuracy: 53.86%\n",
      "Epoch 14/2000, Loss: 0.5946, Test Accuracy: 54.46%\n",
      "âœ… Model saved with accuracy: 54.46%\n",
      "Epoch 15/2000, Loss: 0.5910, Test Accuracy: 55.05%\n",
      "âœ… Model saved with accuracy: 55.05%\n",
      "Epoch 16/2000, Loss: 0.5843, Test Accuracy: 53.66%\n",
      "Epoch 17/2000, Loss: 0.5739, Test Accuracy: 53.27%\n",
      "Epoch 18/2000, Loss: 0.5448, Test Accuracy: 53.86%\n",
      "Epoch 19/2000, Loss: 0.5238, Test Accuracy: 53.07%\n",
      "Epoch 20/2000, Loss: 0.5133, Test Accuracy: 53.66%\n",
      "Epoch 21/2000, Loss: 0.5131, Test Accuracy: 53.47%\n",
      "Epoch 22/2000, Loss: 0.5104, Test Accuracy: 52.87%\n",
      "Epoch 23/2000, Loss: 0.5041, Test Accuracy: 53.47%\n",
      "Epoch 24/2000, Loss: 0.4894, Test Accuracy: 52.67%\n",
      "Epoch 25/2000, Loss: 0.4757, Test Accuracy: 53.07%\n",
      "Epoch 26/2000, Loss: 0.4653, Test Accuracy: 53.47%\n",
      "Epoch 27/2000, Loss: 0.4653, Test Accuracy: 53.27%\n",
      "Epoch 28/2000, Loss: 0.4652, Test Accuracy: 52.67%\n",
      "Epoch 29/2000, Loss: 0.4615, Test Accuracy: 53.27%\n",
      "Epoch 30/2000, Loss: 0.4527, Test Accuracy: 52.87%\n",
      "Epoch 31/2000, Loss: 0.4450, Test Accuracy: 53.47%\n",
      "Epoch 32/2000, Loss: 0.4396, Test Accuracy: 53.27%\n",
      "Epoch 33/2000, Loss: 0.4385, Test Accuracy: 53.47%\n",
      "Epoch 34/2000, Loss: 0.4388, Test Accuracy: 53.07%\n",
      "Epoch 35/2000, Loss: 0.4373, Test Accuracy: 52.67%\n",
      "Epoch 36/2000, Loss: 0.4356, Test Accuracy: 51.68%\n",
      "Epoch 37/2000, Loss: 0.4297, Test Accuracy: 53.07%\n",
      "Epoch 38/2000, Loss: 0.4270, Test Accuracy: 52.28%\n",
      "Epoch 39/2000, Loss: 0.4253, Test Accuracy: 52.08%\n",
      "Epoch 40/2000, Loss: 0.4257, Test Accuracy: 52.08%\n",
      "Epoch 41/2000, Loss: 0.4253, Test Accuracy: 52.08%\n",
      "Epoch 42/2000, Loss: 0.4243, Test Accuracy: 52.28%\n",
      "Epoch 43/2000, Loss: 0.4222, Test Accuracy: 52.48%\n",
      "Epoch 44/2000, Loss: 0.4203, Test Accuracy: 52.67%\n",
      "Epoch 45/2000, Loss: 0.4191, Test Accuracy: 52.48%\n",
      "Epoch 46/2000, Loss: 0.4191, Test Accuracy: 52.67%\n",
      "Epoch 47/2000, Loss: 0.4191, Test Accuracy: 52.48%\n",
      "Epoch 48/2000, Loss: 0.4187, Test Accuracy: 52.48%\n",
      "Epoch 49/2000, Loss: 0.4178, Test Accuracy: 52.48%\n",
      "Epoch 50/2000, Loss: 0.4166, Test Accuracy: 52.48%\n",
      "Epoch 51/2000, Loss: 0.4159, Test Accuracy: 52.48%\n",
      "Epoch 52/2000, Loss: 0.4156, Test Accuracy: 52.48%\n",
      "Epoch 53/2000, Loss: 0.4157, Test Accuracy: 52.87%\n",
      "Epoch 54/2000, Loss: 0.4154, Test Accuracy: 52.48%\n",
      "Epoch 55/2000, Loss: 0.4150, Test Accuracy: 52.67%\n",
      "Epoch 56/2000, Loss: 0.4144, Test Accuracy: 52.87%\n",
      "Epoch 57/2000, Loss: 0.4138, Test Accuracy: 53.07%\n",
      "Epoch 58/2000, Loss: 0.4136, Test Accuracy: 53.07%\n",
      "Epoch 59/2000, Loss: 0.4135, Test Accuracy: 53.07%\n",
      "Epoch 60/2000, Loss: 0.4134, Test Accuracy: 53.07%\n",
      "Epoch 61/2000, Loss: 0.4132, Test Accuracy: 53.07%\n",
      "Epoch 62/2000, Loss: 0.4129, Test Accuracy: 53.07%\n",
      "Epoch 63/2000, Loss: 0.4126, Test Accuracy: 53.07%\n",
      "Epoch 64/2000, Loss: 0.4123, Test Accuracy: 53.07%\n",
      "Epoch 65/2000, Loss: 0.4122, Test Accuracy: 53.07%\n",
      "Epoch 66/2000, Loss: 0.4121, Test Accuracy: 53.07%\n",
      "Epoch 67/2000, Loss: 0.4120, Test Accuracy: 53.07%\n",
      "Epoch 68/2000, Loss: 0.4118, Test Accuracy: 53.07%\n",
      "Epoch 69/2000, Loss: 0.4116, Test Accuracy: 53.07%\n",
      "Epoch 70/2000, Loss: 0.4114, Test Accuracy: 53.07%\n",
      "Epoch 71/2000, Loss: 0.4113, Test Accuracy: 52.87%\n",
      "Epoch 72/2000, Loss: 0.4112, Test Accuracy: 52.87%\n",
      "Epoch 73/2000, Loss: 0.4111, Test Accuracy: 52.87%\n",
      "Epoch 74/2000, Loss: 0.4109, Test Accuracy: 52.87%\n",
      "Epoch 75/2000, Loss: 0.4107, Test Accuracy: 52.87%\n",
      "Epoch 76/2000, Loss: 0.4106, Test Accuracy: 52.87%\n",
      "Epoch 77/2000, Loss: 0.4105, Test Accuracy: 52.87%\n",
      "Epoch 78/2000, Loss: 0.4104, Test Accuracy: 52.87%\n",
      "Epoch 79/2000, Loss: 0.4103, Test Accuracy: 52.87%\n",
      "Epoch 80/2000, Loss: 0.4102, Test Accuracy: 52.87%\n",
      "Epoch 81/2000, Loss: 0.4100, Test Accuracy: 52.87%\n",
      "Epoch 82/2000, Loss: 0.4099, Test Accuracy: 52.87%\n",
      "Epoch 83/2000, Loss: 0.4098, Test Accuracy: 52.87%\n",
      "Epoch 84/2000, Loss: 0.4097, Test Accuracy: 52.67%\n",
      "Epoch 85/2000, Loss: 0.4096, Test Accuracy: 52.67%\n",
      "Epoch 86/2000, Loss: 0.4095, Test Accuracy: 52.67%\n",
      "Epoch 87/2000, Loss: 0.4094, Test Accuracy: 52.67%\n",
      "Epoch 88/2000, Loss: 0.4093, Test Accuracy: 52.67%\n",
      "Epoch 89/2000, Loss: 0.4091, Test Accuracy: 52.67%\n",
      "Epoch 90/2000, Loss: 0.4090, Test Accuracy: 52.67%\n",
      "Epoch 91/2000, Loss: 0.4090, Test Accuracy: 52.67%\n",
      "Epoch 92/2000, Loss: 0.4088, Test Accuracy: 52.67%\n",
      "Epoch 93/2000, Loss: 0.4087, Test Accuracy: 52.67%\n",
      "Epoch 94/2000, Loss: 0.4086, Test Accuracy: 52.87%\n",
      "Epoch 95/2000, Loss: 0.4085, Test Accuracy: 52.67%\n",
      "Epoch 96/2000, Loss: 0.4084, Test Accuracy: 52.87%\n",
      "Epoch 97/2000, Loss: 0.4083, Test Accuracy: 52.67%\n",
      "Epoch 98/2000, Loss: 0.4082, Test Accuracy: 52.87%\n",
      "Epoch 99/2000, Loss: 0.4081, Test Accuracy: 52.87%\n",
      "Epoch 100/2000, Loss: 0.4080, Test Accuracy: 53.07%\n",
      "Epoch 101/2000, Loss: 0.4079, Test Accuracy: 52.87%\n",
      "Epoch 102/2000, Loss: 0.4078, Test Accuracy: 53.07%\n",
      "Epoch 103/2000, Loss: 0.4077, Test Accuracy: 53.07%\n",
      "Epoch 104/2000, Loss: 0.4076, Test Accuracy: 53.07%\n",
      "Epoch 105/2000, Loss: 0.4075, Test Accuracy: 53.07%\n",
      "Epoch 106/2000, Loss: 0.4074, Test Accuracy: 53.07%\n",
      "Epoch 107/2000, Loss: 0.4073, Test Accuracy: 53.07%\n",
      "Epoch 108/2000, Loss: 0.4072, Test Accuracy: 53.07%\n",
      "Epoch 109/2000, Loss: 0.4071, Test Accuracy: 53.07%\n",
      "Epoch 110/2000, Loss: 0.4070, Test Accuracy: 53.07%\n",
      "Epoch 111/2000, Loss: 0.4069, Test Accuracy: 53.07%\n",
      "Epoch 112/2000, Loss: 0.4068, Test Accuracy: 53.07%\n",
      "Epoch 113/2000, Loss: 0.4067, Test Accuracy: 53.07%\n",
      "Epoch 114/2000, Loss: 0.4066, Test Accuracy: 53.07%\n",
      "Epoch 115/2000, Loss: 0.4065, Test Accuracy: 53.07%\n",
      "Epoch 116/2000, Loss: 0.4064, Test Accuracy: 53.07%\n",
      "Epoch 117/2000, Loss: 0.4063, Test Accuracy: 53.07%\n",
      "Epoch 118/2000, Loss: 0.4061, Test Accuracy: 53.07%\n",
      "Epoch 119/2000, Loss: 0.4060, Test Accuracy: 53.07%\n",
      "Epoch 120/2000, Loss: 0.4059, Test Accuracy: 53.07%\n",
      "Epoch 121/2000, Loss: 0.4059, Test Accuracy: 52.87%\n",
      "Epoch 122/2000, Loss: 0.4057, Test Accuracy: 53.07%\n",
      "Epoch 123/2000, Loss: 0.4057, Test Accuracy: 52.87%\n",
      "Epoch 124/2000, Loss: 0.4055, Test Accuracy: 52.87%\n",
      "Epoch 125/2000, Loss: 0.4054, Test Accuracy: 52.87%\n",
      "Epoch 126/2000, Loss: 0.4053, Test Accuracy: 52.87%\n",
      "Epoch 127/2000, Loss: 0.4052, Test Accuracy: 52.67%\n",
      "Epoch 128/2000, Loss: 0.4051, Test Accuracy: 52.67%\n",
      "Epoch 129/2000, Loss: 0.4050, Test Accuracy: 52.67%\n",
      "Epoch 130/2000, Loss: 0.4049, Test Accuracy: 52.87%\n",
      "Epoch 131/2000, Loss: 0.4048, Test Accuracy: 52.87%\n",
      "Epoch 132/2000, Loss: 0.4047, Test Accuracy: 52.48%\n",
      "Epoch 133/2000, Loss: 0.4046, Test Accuracy: 52.48%\n",
      "Epoch 134/2000, Loss: 0.4045, Test Accuracy: 52.67%\n",
      "Epoch 135/2000, Loss: 0.4044, Test Accuracy: 52.48%\n",
      "Epoch 136/2000, Loss: 0.4043, Test Accuracy: 52.67%\n",
      "Epoch 137/2000, Loss: 0.4042, Test Accuracy: 52.48%\n",
      "Epoch 138/2000, Loss: 0.4041, Test Accuracy: 52.48%\n",
      "Epoch 139/2000, Loss: 0.4040, Test Accuracy: 52.48%\n",
      "Epoch 140/2000, Loss: 0.4039, Test Accuracy: 52.48%\n",
      "Epoch 141/2000, Loss: 0.4038, Test Accuracy: 52.48%\n",
      "Epoch 142/2000, Loss: 0.4037, Test Accuracy: 52.48%\n",
      "Epoch 143/2000, Loss: 0.4036, Test Accuracy: 52.48%\n",
      "Epoch 144/2000, Loss: 0.4035, Test Accuracy: 52.48%\n",
      "Epoch 145/2000, Loss: 0.4034, Test Accuracy: 52.28%\n",
      "Epoch 146/2000, Loss: 0.4033, Test Accuracy: 52.28%\n",
      "Epoch 147/2000, Loss: 0.4032, Test Accuracy: 52.28%\n",
      "Epoch 148/2000, Loss: 0.4031, Test Accuracy: 52.48%\n",
      "Epoch 149/2000, Loss: 0.4030, Test Accuracy: 52.28%\n",
      "Epoch 150/2000, Loss: 0.4029, Test Accuracy: 52.28%\n",
      "Epoch 151/2000, Loss: 0.4028, Test Accuracy: 52.28%\n",
      "Epoch 152/2000, Loss: 0.4027, Test Accuracy: 52.28%\n",
      "Epoch 153/2000, Loss: 0.4026, Test Accuracy: 52.28%\n",
      "Epoch 154/2000, Loss: 0.4025, Test Accuracy: 52.28%\n",
      "Epoch 155/2000, Loss: 0.4024, Test Accuracy: 52.28%\n",
      "Epoch 156/2000, Loss: 0.4022, Test Accuracy: 52.28%\n",
      "Epoch 157/2000, Loss: 0.4022, Test Accuracy: 52.28%\n",
      "Epoch 158/2000, Loss: 0.4020, Test Accuracy: 52.28%\n",
      "Epoch 159/2000, Loss: 0.4020, Test Accuracy: 52.28%\n",
      "Epoch 160/2000, Loss: 0.4019, Test Accuracy: 52.28%\n",
      "Epoch 161/2000, Loss: 0.4018, Test Accuracy: 52.28%\n",
      "Epoch 162/2000, Loss: 0.4017, Test Accuracy: 52.28%\n",
      "Epoch 163/2000, Loss: 0.4015, Test Accuracy: 52.28%\n",
      "Epoch 164/2000, Loss: 0.4014, Test Accuracy: 52.28%\n",
      "Epoch 165/2000, Loss: 0.4013, Test Accuracy: 52.28%\n",
      "Epoch 166/2000, Loss: 0.4013, Test Accuracy: 52.28%\n",
      "Epoch 167/2000, Loss: 0.4011, Test Accuracy: 52.28%\n",
      "Epoch 168/2000, Loss: 0.4010, Test Accuracy: 52.28%\n",
      "Epoch 169/2000, Loss: 0.4009, Test Accuracy: 52.28%\n",
      "Epoch 170/2000, Loss: 0.4008, Test Accuracy: 52.28%\n",
      "Epoch 171/2000, Loss: 0.4007, Test Accuracy: 52.28%\n",
      "Epoch 172/2000, Loss: 0.4006, Test Accuracy: 52.28%\n",
      "Epoch 173/2000, Loss: 0.4005, Test Accuracy: 52.28%\n",
      "Epoch 174/2000, Loss: 0.4004, Test Accuracy: 52.28%\n",
      "Epoch 175/2000, Loss: 0.4003, Test Accuracy: 52.28%\n",
      "Epoch 176/2000, Loss: 0.4002, Test Accuracy: 52.28%\n",
      "Epoch 177/2000, Loss: 0.4001, Test Accuracy: 52.28%\n",
      "Epoch 178/2000, Loss: 0.4000, Test Accuracy: 52.28%\n",
      "Epoch 179/2000, Loss: 0.3999, Test Accuracy: 52.28%\n",
      "Epoch 180/2000, Loss: 0.3998, Test Accuracy: 52.28%\n",
      "Epoch 181/2000, Loss: 0.3997, Test Accuracy: 52.28%\n",
      "Epoch 182/2000, Loss: 0.3996, Test Accuracy: 52.28%\n",
      "Epoch 183/2000, Loss: 0.3995, Test Accuracy: 52.28%\n",
      "Epoch 184/2000, Loss: 0.3994, Test Accuracy: 52.28%\n",
      "Epoch 185/2000, Loss: 0.3993, Test Accuracy: 52.28%\n",
      "Epoch 186/2000, Loss: 0.3992, Test Accuracy: 52.28%\n",
      "Epoch 187/2000, Loss: 0.3991, Test Accuracy: 52.28%\n",
      "Epoch 188/2000, Loss: 0.3990, Test Accuracy: 52.28%\n",
      "Epoch 189/2000, Loss: 0.3988, Test Accuracy: 52.28%\n",
      "Epoch 190/2000, Loss: 0.3987, Test Accuracy: 52.28%\n",
      "Epoch 191/2000, Loss: 0.3986, Test Accuracy: 52.28%\n",
      "Epoch 192/2000, Loss: 0.3985, Test Accuracy: 52.28%\n",
      "Epoch 193/2000, Loss: 0.3984, Test Accuracy: 52.28%\n",
      "Epoch 194/2000, Loss: 0.3983, Test Accuracy: 52.28%\n",
      "Epoch 195/2000, Loss: 0.3982, Test Accuracy: 52.28%\n",
      "Epoch 196/2000, Loss: 0.3981, Test Accuracy: 52.28%\n",
      "Epoch 197/2000, Loss: 0.3980, Test Accuracy: 52.28%\n",
      "Epoch 198/2000, Loss: 0.3979, Test Accuracy: 52.28%\n",
      "Epoch 199/2000, Loss: 0.3978, Test Accuracy: 52.28%\n",
      "Epoch 200/2000, Loss: 0.3977, Test Accuracy: 52.28%\n",
      "Epoch 201/2000, Loss: 0.3976, Test Accuracy: 52.28%\n",
      "Epoch 202/2000, Loss: 0.3975, Test Accuracy: 52.28%\n",
      "Epoch 203/2000, Loss: 0.3974, Test Accuracy: 52.28%\n",
      "Epoch 204/2000, Loss: 0.3973, Test Accuracy: 52.28%\n",
      "Epoch 205/2000, Loss: 0.3972, Test Accuracy: 52.28%\n",
      "Epoch 206/2000, Loss: 0.3971, Test Accuracy: 52.28%\n",
      "Epoch 207/2000, Loss: 0.3969, Test Accuracy: 52.28%\n",
      "Epoch 208/2000, Loss: 0.3969, Test Accuracy: 52.28%\n",
      "Epoch 209/2000, Loss: 0.3967, Test Accuracy: 52.28%\n",
      "Epoch 210/2000, Loss: 0.3966, Test Accuracy: 52.28%\n",
      "Epoch 211/2000, Loss: 0.3965, Test Accuracy: 52.28%\n",
      "Epoch 212/2000, Loss: 0.3964, Test Accuracy: 52.28%\n",
      "Epoch 213/2000, Loss: 0.3963, Test Accuracy: 52.28%\n",
      "Epoch 214/2000, Loss: 0.3962, Test Accuracy: 52.28%\n",
      "Epoch 215/2000, Loss: 0.3961, Test Accuracy: 52.28%\n",
      "ðŸ›‘ Early stopping at epoch 215\n",
      "\n",
      "ðŸ”¹ Loading best model for final evaluation...\n",
      "ðŸ† Final Test Accuracy: 55.05%\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "# === 1. Ð—ÐÐ“Ð Ð£Ð—ÐšÐ Ð”ÐÐÐÐ«Ð¥ ===\n",
    "db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\RTS_futures_options_day.db')\n",
    "\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    df_fut = pd.read_sql_query(\n",
    "        \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Futures\",\n",
    "        conn\n",
    "    )\n",
    "\n",
    "# === 2. ÐšÐžÐ”Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• Ð¡Ð’Ð•Ð§Ð•Ð™ (ÐœÐ•Ð¢ÐžÐ” Ð›Ð˜Ð¥ÐžÐ’Ð˜Ð”ÐžÐ’Ð) ===\n",
    "def encode_candle(row):\n",
    "    open_, low, high, close = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE']\n",
    "    direction = 1 if close > open_ else 0 if close < open_ else 2\n",
    "    upper_shadow = high - max(open_, close)\n",
    "    lower_shadow = min(open_, close) - low\n",
    "    body = abs(close - open_)\n",
    "\n",
    "    def classify_shadow(shadow, body):\n",
    "        if shadow < 0.1 * body:\n",
    "            return 0  \n",
    "        elif shadow < 0.5 * body:\n",
    "            return 1  \n",
    "        else:\n",
    "            return 2  \n",
    "\n",
    "    upper_code = classify_shadow(upper_shadow, body)\n",
    "    lower_code = classify_shadow(lower_shadow, body)\n",
    "\n",
    "    return f\"{direction}{upper_code}{lower_code}\"\n",
    "\n",
    "df_fut['CANDLE_CODE'] = df_fut.apply(encode_candle, axis=1)\n",
    "\n",
    "# === 3. ÐŸÐžÐ”Ð“ÐžÐ¢ÐžÐ’ÐšÐ Ð”ÐÐÐÐ«Ð¥ ===\n",
    "unique_codes = sorted(df_fut['CANDLE_CODE'].unique())\n",
    "code_to_int = {code: i for i, code in enumerate(unique_codes)}\n",
    "df_fut['CANDLE_INT'] = df_fut['CANDLE_CODE'].map(code_to_int)\n",
    "\n",
    "window_size = 20  \n",
    "predict_offset = 1  \n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df_fut) - window_size - predict_offset):\n",
    "    X.append(df_fut['CANDLE_INT'].iloc[i:i+window_size].values)\n",
    "    y.append(1 if df_fut['CLOSE'].iloc[i+window_size+predict_offset] > df_fut['CLOSE'].iloc[i+window_size] else 0)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "class CandlestickDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = CandlestickDataset(X_train, y_train)\n",
    "test_dataset = CandlestickDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === 4. Ð¡ÐžÐ—Ð”ÐÐÐ˜Ð• ÐÐ•Ð™Ð ÐžÐ¡Ð•Ð¢Ð˜ (LSTM) ===\n",
    "class CandleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super(CandleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# === 5. ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• Ð¡ Ð ÐÐÐÐ•Ð™ ÐžÐ¡Ð¢ÐÐÐžÐ’ÐšÐžÐ™, CLR Ð¸ WEIGHT DECAY ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CandleLSTM(vocab_size=len(unique_codes), embedding_dim=8, hidden_dim=32, output_dim=1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-2, step_size_up=200, mode=\"triangular2\")\n",
    "\n",
    "best_accuracy = 0  \n",
    "model_path = \"best_model.pth\"\n",
    "early_stop_epochs = 200  \n",
    "epochs_no_improve = 0\n",
    "\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # === Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ===\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch).squeeze().round()\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # === Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ===\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"âœ… Model saved with accuracy: {best_accuracy:.2%}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_epochs:\n",
    "        print(f\"ðŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# === 6. Ð—ÐÐ“Ð Ð£Ð—ÐšÐ Ð›Ð£Ð§Ð¨Ð•Ð™ ÐœÐžÐ”Ð•Ð›Ð˜ Ð˜ Ð¢Ð•Ð¡Ð¢ ===\n",
    "print(\"\\nðŸ”¹ Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch).squeeze().round()\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "final_accuracy = correct / total\n",
    "print(f\"ðŸ† Final Test Accuracy: {final_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ðŸ“‰ DOWN (Probability: 25.93%)\n"
     ]
    }
   ],
   "source": [
    "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 20 ÑÐ²ÐµÑ‡ÐµÐ¹ Ð¸Ð· df_fut\n",
    "last_sequence = torch.tensor(df_fut['CANDLE_INT'].iloc[-20:].values, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ\n",
    "with torch.no_grad():\n",
    "    prediction = model(last_sequence).item()\n",
    "    direction = \"ðŸ“ˆ UP\" if prediction > 0.5 else \"ðŸ“‰ DOWN\"\n",
    "    print(f\"Prediction: {direction} (Probability: {prediction:.2%})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
