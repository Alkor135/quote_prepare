{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÐšÐ¾Ð´Ð¸Ñ€Ð¾Ð²ÐºÐ° ÑÐ²ÐµÑ‡ÐµÐ¹ Ð¿Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ñƒ Ð›Ð¸Ñ…Ð¾Ð²Ð¸Ð´Ð¾Ð²Ð°, Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð· Up Ð¸Ð»Ð¸ Down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ¾Ð´Ð° ÑÐ²ÐµÑ‡Ð¸ Ð›Ð¸Ñ…Ð¾Ð²Ð¸Ð´Ð¾Ð²Ð°.  \n",
    "Ð‘Ð¸Ð½Ð°Ñ€Ð½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Up Ð¸Ð»Ð¸ Down.  \n",
    "ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ seed. ÐŸÐ¾Ð´Ð±Ð¸Ñ€Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°Ð¼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Loss: 0.6942, Test Accuracy: 53.85%, Best accuracy: 0.00%, Epoch best accuracy: 0, seed: 97\n",
      "âœ… Model saved with accuracy: 53.85%\n",
      "Epoch 2/2000, Loss: 0.6920, Test Accuracy: 52.66%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 3/2000, Loss: 0.6908, Test Accuracy: 52.66%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 4/2000, Loss: 0.6902, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 5/2000, Loss: 0.6901, Test Accuracy: 53.06%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 6/2000, Loss: 0.6883, Test Accuracy: 52.66%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 7/2000, Loss: 0.6886, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 8/2000, Loss: 0.6886, Test Accuracy: 52.27%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 9/2000, Loss: 0.6858, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 10/2000, Loss: 0.6849, Test Accuracy: 52.47%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 11/2000, Loss: 0.6829, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 12/2000, Loss: 0.6810, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 13/2000, Loss: 0.6776, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 14/2000, Loss: 0.6742, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 15/2000, Loss: 0.6695, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 16/2000, Loss: 0.6666, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 17/2000, Loss: 0.6617, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 18/2000, Loss: 0.6602, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 19/2000, Loss: 0.6533, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 20/2000, Loss: 0.6471, Test Accuracy: 48.72%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 21/2000, Loss: 0.6400, Test Accuracy: 48.13%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 22/2000, Loss: 0.6369, Test Accuracy: 47.93%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 23/2000, Loss: 0.6283, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 24/2000, Loss: 0.6246, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 25/2000, Loss: 0.6188, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 26/2000, Loss: 0.6132, Test Accuracy: 48.32%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 27/2000, Loss: 0.6041, Test Accuracy: 49.31%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 28/2000, Loss: 0.6003, Test Accuracy: 47.73%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 29/2000, Loss: 0.5921, Test Accuracy: 48.72%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 30/2000, Loss: 0.5828, Test Accuracy: 48.52%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 31/2000, Loss: 0.5725, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 32/2000, Loss: 0.5704, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 33/2000, Loss: 0.5570, Test Accuracy: 48.72%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 34/2000, Loss: 0.5550, Test Accuracy: 48.52%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 35/2000, Loss: 0.5438, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 36/2000, Loss: 0.5412, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 37/2000, Loss: 0.5316, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 38/2000, Loss: 0.5233, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 39/2000, Loss: 0.5134, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 40/2000, Loss: 0.5092, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 41/2000, Loss: 0.5017, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 42/2000, Loss: 0.4971, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 43/2000, Loss: 0.4941, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 44/2000, Loss: 0.4849, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 45/2000, Loss: 0.4794, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 46/2000, Loss: 0.4776, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 47/2000, Loss: 0.4672, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 48/2000, Loss: 0.4579, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 49/2000, Loss: 0.4520, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 50/2000, Loss: 0.4521, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 51/2000, Loss: 0.4380, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 52/2000, Loss: 0.4478, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 53/2000, Loss: 0.4255, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 54/2000, Loss: 0.4197, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 55/2000, Loss: 0.4096, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 56/2000, Loss: 0.4020, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 57/2000, Loss: 0.3973, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 58/2000, Loss: 0.3934, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 59/2000, Loss: 0.3839, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 60/2000, Loss: 0.3791, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 61/2000, Loss: 0.3733, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 62/2000, Loss: 0.3627, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 63/2000, Loss: 0.3563, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 64/2000, Loss: 0.3500, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 65/2000, Loss: 0.3376, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 66/2000, Loss: 0.3339, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 67/2000, Loss: 0.3252, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 68/2000, Loss: 0.3233, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 69/2000, Loss: 0.3119, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 70/2000, Loss: 0.3036, Test Accuracy: 52.27%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 71/2000, Loss: 0.2975, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 72/2000, Loss: 0.2907, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 73/2000, Loss: 0.2922, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 74/2000, Loss: 0.2842, Test Accuracy: 52.27%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 75/2000, Loss: 0.2821, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 76/2000, Loss: 0.2679, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 77/2000, Loss: 0.2597, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 78/2000, Loss: 0.2536, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 79/2000, Loss: 0.2493, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 80/2000, Loss: 0.2452, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 81/2000, Loss: 0.2301, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 82/2000, Loss: 0.2300, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 83/2000, Loss: 0.2176, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 84/2000, Loss: 0.2095, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 85/2000, Loss: 0.2197, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 86/2000, Loss: 0.2137, Test Accuracy: 48.52%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 87/2000, Loss: 0.2148, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 88/2000, Loss: 0.2012, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 89/2000, Loss: 0.1908, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 90/2000, Loss: 0.1860, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 91/2000, Loss: 0.1739, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 92/2000, Loss: 0.1706, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 93/2000, Loss: 0.2017, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 94/2000, Loss: 0.1909, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 95/2000, Loss: 0.1756, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 96/2000, Loss: 0.1638, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 97/2000, Loss: 0.1458, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 98/2000, Loss: 0.1522, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 99/2000, Loss: 0.1436, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 100/2000, Loss: 0.1383, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 101/2000, Loss: 0.1442, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 102/2000, Loss: 0.1452, Test Accuracy: 49.31%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 103/2000, Loss: 0.1409, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 104/2000, Loss: 0.1336, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 105/2000, Loss: 0.1441, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 106/2000, Loss: 0.1210, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 107/2000, Loss: 0.1150, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 108/2000, Loss: 0.1058, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 109/2000, Loss: 0.1038, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 110/2000, Loss: 0.0993, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 111/2000, Loss: 0.0966, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 112/2000, Loss: 0.0962, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 113/2000, Loss: 0.0934, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 114/2000, Loss: 0.0870, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 115/2000, Loss: 0.1217, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 116/2000, Loss: 0.0914, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 117/2000, Loss: 0.0944, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 118/2000, Loss: 0.0826, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 119/2000, Loss: 0.0806, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 120/2000, Loss: 0.0999, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 121/2000, Loss: 0.2397, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 122/2000, Loss: 0.2066, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 123/2000, Loss: 0.1879, Test Accuracy: 52.66%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 124/2000, Loss: 0.1060, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 125/2000, Loss: 0.0805, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 126/2000, Loss: 0.0709, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 127/2000, Loss: 0.0664, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 128/2000, Loss: 0.0645, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 129/2000, Loss: 0.0614, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 130/2000, Loss: 0.0577, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 131/2000, Loss: 0.0564, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 132/2000, Loss: 0.0542, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 133/2000, Loss: 0.0511, Test Accuracy: 52.27%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 134/2000, Loss: 0.0500, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 135/2000, Loss: 0.0476, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 136/2000, Loss: 0.0463, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 137/2000, Loss: 0.0452, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 138/2000, Loss: 0.0428, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 139/2000, Loss: 0.0413, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 140/2000, Loss: 0.0400, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 141/2000, Loss: 0.0409, Test Accuracy: 50.69%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 142/2000, Loss: 0.1131, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 143/2000, Loss: 0.2509, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 144/2000, Loss: 0.1896, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 145/2000, Loss: 0.0983, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 146/2000, Loss: 0.0713, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 147/2000, Loss: 0.0784, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 148/2000, Loss: 0.0762, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 149/2000, Loss: 0.0527, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 150/2000, Loss: 0.0421, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 151/2000, Loss: 0.0375, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 152/2000, Loss: 0.0346, Test Accuracy: 51.68%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 153/2000, Loss: 0.0330, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 154/2000, Loss: 0.0314, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 155/2000, Loss: 0.0304, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 156/2000, Loss: 0.0289, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 157/2000, Loss: 0.0281, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 158/2000, Loss: 0.0272, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 159/2000, Loss: 0.0255, Test Accuracy: 51.48%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 160/2000, Loss: 0.0254, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 161/2000, Loss: 0.0236, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 162/2000, Loss: 0.0231, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 163/2000, Loss: 0.0220, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 164/2000, Loss: 0.0212, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 165/2000, Loss: 0.0203, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 166/2000, Loss: 0.0197, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 167/2000, Loss: 0.0190, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 168/2000, Loss: 0.0186, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 169/2000, Loss: 0.0177, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 170/2000, Loss: 0.0178, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 171/2000, Loss: 0.0169, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 172/2000, Loss: 0.0163, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 173/2000, Loss: 0.0156, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 174/2000, Loss: 0.0147, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 175/2000, Loss: 0.0142, Test Accuracy: 49.31%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 176/2000, Loss: 0.0135, Test Accuracy: 49.90%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 177/2000, Loss: 0.0131, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 178/2000, Loss: 0.0127, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 179/2000, Loss: 0.0124, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 180/2000, Loss: 0.0119, Test Accuracy: 50.49%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 181/2000, Loss: 0.0112, Test Accuracy: 50.10%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 182/2000, Loss: 0.0108, Test Accuracy: 49.31%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 183/2000, Loss: 0.0107, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 184/2000, Loss: 0.0101, Test Accuracy: 49.11%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 185/2000, Loss: 0.0096, Test Accuracy: 49.31%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 186/2000, Loss: 0.0094, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 187/2000, Loss: 0.0089, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 188/2000, Loss: 0.0087, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 189/2000, Loss: 0.1348, Test Accuracy: 50.89%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 190/2000, Loss: 0.6098, Test Accuracy: 51.87%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 191/2000, Loss: 0.2447, Test Accuracy: 52.07%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 192/2000, Loss: 0.1405, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 193/2000, Loss: 0.1243, Test Accuracy: 51.28%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 194/2000, Loss: 0.0907, Test Accuracy: 49.70%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 195/2000, Loss: 0.0510, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 196/2000, Loss: 0.0414, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 197/2000, Loss: 0.0272, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 198/2000, Loss: 0.0241, Test Accuracy: 49.51%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 199/2000, Loss: 0.0309, Test Accuracy: 48.92%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 200/2000, Loss: 0.0279, Test Accuracy: 50.30%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "Epoch 201/2000, Loss: 0.0253, Test Accuracy: 51.08%, Best accuracy: 53.85%, Epoch best accuracy: 1, seed: 97\n",
      "ðŸ›‘ Early stopping at epoch 201\n",
      "\n",
      "ðŸ”¹ Loading best model for final evaluation...\n",
      "ðŸ† Final Test Accuracy: 53.85%\n"
     ]
    }
   ],
   "source": [
    "# === 1. Ð¤Ð˜ÐšÐ¡ÐÐ¦Ð˜Ð¯ Ð¡Ð›Ð£Ð§ÐÐ™ÐÐ«Ð¥ Ð§Ð˜Ð¡Ð•Ð› Ð”Ð›Ð¯ Ð”Ð•Ð¢Ð•Ð ÐœÐ˜ÐÐ˜Ð ÐžÐ’ÐÐÐÐžÐ¡Ð¢Ð˜ ===\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# [64, 97], 2, 5, 16, 22, 32, 34, 51, 64, 75, 89, 97\n",
    "seed_var = 97\n",
    "set_seed(seed_var)  # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ð¹ seed\n",
    "\n",
    "# === 2. Ð—ÐÐ“Ð Ð£Ð—ÐšÐ Ð”ÐÐÐÐ«Ð¥ ===\n",
    "# db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\RTS_futures_options_day.db')\n",
    "\n",
    "# with sqlite3.connect(db_path) as conn:\n",
    "#     df_fut = pd.read_sql_query(\n",
    "#         \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Futures\",\n",
    "#         conn\n",
    "#     )\n",
    "\n",
    "db_path = Path(r'C:\\Users\\Alkor\\gd\\data_quote_db\\MIX_futures_day.db')\n",
    "\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    df_fut = pd.read_sql_query(\n",
    "        \"SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME FROM Day\",\n",
    "        conn\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Ð¡Ñ‚Ñ€Ð¾ÐºÐ° Ð½Ð¸Ð¶Ðµ.\n",
    "Ð¤Ð¸ÐºÑÐ°Ñ†Ð¸Ñ Ð¿Ð¾Ñ€ÑÐ´ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ). Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ñ‚.Ðº. \n",
    "Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ„Ð¸Ñ‡ÐµÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… ÑÐ²ÐµÑ‡ÐµÐ¹.\n",
    "\"\"\"\n",
    "# df_fut = df_fut.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === 3. Ð¤Ð£ÐÐšÐ¦Ð˜Ð¯ ÐšÐžÐ”Ð˜Ð ÐžÐ’ÐÐÐ˜Ð¯ Ð¡Ð’Ð•Ð§Ð•Ð™ (Ð›Ð˜Ð¥ÐžÐ’Ð˜Ð”ÐžÐ’) ===\n",
    "def encode_candle(row):\n",
    "    open_, low, high, close = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE']\n",
    "\n",
    "    direction = 1 if close > open_ else (0 if close < open_ else 2)\n",
    "    upper_shadow = high - max(open_, close)\n",
    "    lower_shadow = min(open_, close) - low\n",
    "    body = abs(close - open_)\n",
    "\n",
    "    def classify_shadow(shadow, body):\n",
    "        return 0 if shadow < 0.1 * body else (1 if shadow < 0.5 * body else 2)\n",
    "\n",
    "    return f\"{direction}{classify_shadow(upper_shadow, body)}{classify_shadow(lower_shadow, body)}\"\n",
    "\n",
    "df_fut['CANDLE_CODE'] = df_fut.apply(encode_candle, axis=1)\n",
    "\n",
    "# === 4. ÐŸÐžÐ”Ð“ÐžÐ¢ÐžÐ’ÐšÐ Ð”ÐÐÐÐ«Ð¥ ===\n",
    "unique_codes = sorted(df_fut['CANDLE_CODE'].unique())\n",
    "code_to_int = {code: i for i, code in enumerate(unique_codes)}\n",
    "df_fut['CANDLE_INT'] = df_fut['CANDLE_CODE'].map(code_to_int)\n",
    "\n",
    "window_size = 20  \n",
    "predict_offset = 1  \n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df_fut) - window_size - predict_offset):\n",
    "    X.append(df_fut['CANDLE_INT'].iloc[i:i+window_size].values)\n",
    "    y.append(\n",
    "        1 if df_fut['CLOSE'].iloc[i+window_size+predict_offset] > \n",
    "        df_fut['CLOSE'].iloc[i+window_size] else 0\n",
    "    )\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "class CandlestickDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(42 + worker_id)\n",
    "    random.seed(42 + worker_id)\n",
    "\n",
    "train_dataset = CandlestickDataset(X_train, y_train)\n",
    "test_dataset = CandlestickDataset(X_test, y_test)\n",
    "# print(X_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "# === 5. Ð¡ÐžÐ—Ð”ÐÐÐ˜Ð• ÐÐ•Ð™Ð ÐžÐ¡Ð•Ð¢Ð˜ (LSTM) ===\n",
    "class CandleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(CandleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# === 6. ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ÐœÐžÐ”Ð•Ð›Ð˜ Ð¡ Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð•Ðœ Ð›Ð£Ð§Ð¨Ð•Ð™ ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CandleLSTM(\n",
    "    vocab_size=len(unique_codes), embedding_dim=8, hidden_dim=32, output_dim=1\n",
    ").to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_accuracy = 0  \n",
    "epoch_best_accuracy = 0\n",
    "model_path = \"best_model_second.pth\"\n",
    "early_stop_epochs = 200  # Ð”Ð»Ñ Ñ€Ð°Ð½Ð½ÐµÐ¹ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸\n",
    "epochs_no_improve = 0\n",
    "\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # === ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ñ‚ÐµÑÑ‚Ðµ Ð¿Ð¾ÑÐ»Ðµ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ð¸ ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch).squeeze().round()\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}, \"\n",
    "        f\"Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "        f\"Test Accuracy: {accuracy:.2%}, \"\n",
    "        f\"Best accuracy: {best_accuracy:.2%}, \"\n",
    "        f\"Epoch best accuracy: {epoch_best_accuracy}, \"\n",
    "        f\"seed: {seed_var}\"\n",
    "    )\n",
    "\n",
    "    # === Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ===\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_no_improve = 0\n",
    "        epoch_best_accuracy = epoch + 1\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"âœ… Model saved with accuracy: {best_accuracy:.2%}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # === Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ===\n",
    "    if epochs_no_improve >= early_stop_epochs:\n",
    "        print(f\"ðŸ›‘ Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# === 7. Ð—ÐÐ“Ð Ð£Ð—ÐšÐ Ð›Ð£Ð§Ð¨Ð•Ð™ ÐœÐžÐ”Ð•Ð›Ð˜ Ð˜ Ð¢Ð•Ð¡Ð¢ ===\n",
    "print(\"\\nðŸ”¹ Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch).squeeze().round()\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "final_accuracy = correct / total\n",
    "print(f\"ðŸ† Final Test Accuracy: {final_accuracy:.2%}\")\n",
    "\n",
    "# # === 8. Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’Ð•Ð Ð¡Ð˜Ð™ Ð‘Ð˜Ð‘Ð›Ð˜ÐžÐ¢Ð•Ðš ===\n",
    "# print(\"\\nðŸ”¹ Library versions:\")\n",
    "# print(f\"PyTorch: {torch.__version__}\")\n",
    "# print(f\"NumPy: {np.__version__}\")\n",
    "# print(f\"Pandas: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ ÐšÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ?  \n",
    "ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÐ²ÐµÑ‡Ð¸:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ðŸ“ˆ UP\n",
      "UP Probability: 53.05%, \n",
      "DOWN Probability: 46.95%\n"
     ]
    }
   ],
   "source": [
    "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "model.load_state_dict(torch.load(\"best_model_second.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 20 ÑÐ²ÐµÑ‡ÐµÐ¹ Ð¸Ð· df_fut\n",
    "last_sequence = torch.tensor(\n",
    "    df_fut['CANDLE_INT'].iloc[-20:].values, dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "# ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ\n",
    "with torch.no_grad():\n",
    "    probability_up = model(last_sequence).item()  # Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ñ€Ð¾ÑÑ‚Ð°\n",
    "    probability_down = 1 - probability_up  # Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ\n",
    "\n",
    "    direction = \"ðŸ“ˆ UP\" if probability_up >= 0.5 else \"ðŸ“‰ DOWN\"\n",
    "\n",
    "    print(f\"Prediction: {direction}\")\n",
    "    print(\n",
    "        f\"UP Probability: {probability_up:.2%}, \\n\"\n",
    "        f\"DOWN Probability: {probability_down:.2%}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
