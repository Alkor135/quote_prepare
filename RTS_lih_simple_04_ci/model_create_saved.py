import sqlite3
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import random
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from sklearn.utils import resample
import json
import os


# === –°–û–ó–î–ê–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (LSTM) ===
class CandleLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(CandleLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])
        return self.sigmoid(x)


class CandlestickDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
    

def seed_worker(worker_id):
    np.random.seed(42 + worker_id)
    random.seed(42 + worker_id)


# === –§–ò–ö–°–ê–¶–ò–Ø –°–õ–£–ß–ê–ô–ù–´–• –ß–ò–°–ï–õ –î–õ–Ø –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–û–°–¢–ò ===
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# === –§–£–ù–ö–¶–ò–Ø –ö–û–î–ò–†–û–í–ê–ù–ò–Ø –°–í–ï–ß–ï–ô (–õ–ò–•–û–í–ò–î–û–í) ===
def encode_candle(row):
    open_, low, high, close = row['OPEN'], row['LOW'], row['HIGH'], row['CLOSE']

    if close > open_:
        direction = 1  # –ë—ã—á—å—è —Å–≤–µ—á–∞
    elif close < open_:
        direction = 0  # –ú–µ–¥–≤–µ–∂—å—è —Å–≤–µ—á–∞
    else:
        direction = 2  # –î–æ–∂–∏

    upper_shadow = high - max(open_, close)
    lower_shadow = min(open_, close) - low
    body = abs(close - open_)

    def classify_shadow(shadow, body):
        if shadow < 0.1 * body:
            return 0  
        elif shadow < 0.5 * body:
            return 1  
        else:
            return 2  

    upper_code = classify_shadow(upper_shadow, body)
    lower_code = classify_shadow(lower_shadow, body)

    return f"{direction}{upper_code}{lower_code}"


# === –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á–µ—Ç–∞ P/L (–ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é) ===
def calculate_pnl(y_preds, open_prices, close_prices):
    pnl = 0
    for i in range(len(y_preds)):
        if y_preds[i] > 0.5:  # –ü–æ–∫—É–ø–∫–∞ (LONG)
            pnl += close_prices[i] - open_prices[i]
        else:  # –ü—Ä–æ–¥–∞–∂–∞ (SHORT)
            pnl += open_prices[i] - close_prices[i]
    return pnl  # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å


# === 1. –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø ===
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø–∞–ø–∫—É, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ñ–∞–π–ª —Å–∫—Ä–∏–ø—Ç–∞
script_dir = Path(__file__).parent
os.chdir(script_dir)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ —Å–≤–µ—á–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
with open("code_full_int.json", "r") as f:
    code_to_int = json.load(f)

db_path = Path(r'C:\Users\Alkor\gd\data_quote_db\RTS_futures_options_day_2014.db')

for counter in range(101, 201):
    set_seed(counter)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π seed

    # === 2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ò –í–ê–õ–ò–î–ê–¶–ò–ò ===
    with sqlite3.connect(db_path) as conn:
        df_fut = pd.read_sql_query(
            """
            SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME 
            FROM Futuures 
            WHERE TRADEDATE BETWEEN '2014-01-01' AND '2024-01-01' 
            ORDER BY TRADEDATE
            """,
            conn
        )

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–¥–æ–≤ —Å–≤–µ—á–µ–π –ø–æ –õ–∏—Ö–æ–≤–∏–¥–æ–≤—É
    df_fut['CANDLE_CODE'] = df_fut.apply(encode_candle, axis=1)

    # === 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–≤–µ—á–Ω—ã–µ –∫–æ–¥—ã –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (—Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–¥–æ–≤)
    df_fut['CANDLE_INT'] = df_fut['CANDLE_CODE'].map(code_to_int)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
    df_fut['DIRECTION'] = (df_fut['CLOSE'] > df_fut['OPEN']).astype(int)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    for i in range(1, 21):
        df_fut[f'CI_{i}'] = df_fut['CANDLE_INT'].shift(i).astype('Int64')

    df_fut = df_fut.dropna().reset_index(drop=True)

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞ —Å–µ—Ç–æ–≤
    feature_columns = [col for col in df_fut.columns if col.startswith('CI')]
    X = df_fut[feature_columns]
    y = df_fut['DIRECTION']
    X, y = np.array(X), np.array(y)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    split = int(0.85 * len(X))
    X_train, y_train = X[:split], y[:split]
    X_test, y_test = X[split:], y[split:]

    # === 4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ === -------------------------------------------------------------
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º X_train –∏ y_train –≤ –æ–¥–∏–Ω DataFrame
    df_train = pd.DataFrame(X_train)
    df_train['TARGET'] = y_train  # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç–∫–æ–π

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    class_counts = df_train['TARGET'].value_counts()
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π:\n", class_counts)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∏ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å—ã
    min_class = class_counts.idxmin()
    max_class = class_counts.idxmax()

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
    df_minority = df_train[df_train['TARGET'] == min_class]
    df_majority = df_train[df_train['TARGET'] == max_class]

    # –£–±–∏—Ä–∞–µ–º –∏–∑ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞—é—Ç 
    # —Å –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–æ–º –ø–æ —Ñ–∏—á–∞–º  
    df_minority_unique = df_minority.loc[  
        ~df_minority.drop(columns=['TARGET']).apply(tuple, axis=1).isin(  
            df_majority.drop(columns=['TARGET']).apply(tuple, axis=1)  
        )]  
    
    # –î—É–±–ª–∏—Ä—É–µ–º –û–¢–§–ò–õ–¨–¢–†–û–í–ê–ù–ù–´–ï –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–¥–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Ä–∞–Ω–¥–æ–º–Ω–æ
    df_minority_upsampled = resample(df_minority_unique,  
                                    replace=True,  
                                    n_samples=len(df_majority),  
                                    random_state=42)  

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±–∞ –∫–ª–∞—Å—Å–∞ –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    df_balanced = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=42)

    # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ X_train –∏ y_train
    X_train = df_balanced.drop(columns=['TARGET']).values
    y_train = df_balanced['TARGET'].values
    X_train = np.array(X_train, dtype=np.int64)  # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
    y_train = np.array(y_train, dtype=np.int64)  # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\n", pd.Series(y_train).value_counts())
    # –ö–æ–Ω–µ—Ü –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ --------------------------------------------------------------------------

    # === 5. –°–û–ó–î–ê–ù–ò–ï DATASET –∏ DATALOADER ===
    X_test = np.array(X_test, dtype=np.int64)  # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
    y_test = np.array(y_test, dtype=np.int64)  # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É

    train_dataset = CandlestickDataset(X_train, y_train)
    test_dataset = CandlestickDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, worker_init_fn=seed_worker)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, worker_init_fn=seed_worker)

    # === 6. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ï–ô –ü–û P/L ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = CandleLSTM(vocab_size=27, embedding_dim=8, hidden_dim=32, output_dim=1).to(device)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    best_pnl = float('-inf')  # –õ—É—á—à–∞—è –ø—Ä–∏–±—ã–ª—å (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ -‚àû)
    epoch_best_pnl = 0
    model_path = Path(fr"model\best_model_{counter}.pth")
    early_stop_epochs = 200
    epochs_no_improve = 0

    epochs = 2000
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(X_batch).squeeze()
            loss = criterion(y_pred, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ ===
        model.eval()
        y_preds = []
        
        with torch.no_grad():
            for X_batch, _ in test_loader:
                X_batch = X_batch.to(device)
                y_pred = model(X_batch).squeeze().cpu().numpy()
                y_preds.extend(y_pred)

        # === –†–∞—Å—á–µ—Ç P/L ===
        test_open_prices = df_fut['OPEN'].iloc[split:].values
        test_close_prices = df_fut['CLOSE'].iloc[split:].values
        pnl = calculate_pnl(y_preds, test_open_prices, test_close_prices)

        print(
            f"Epoch {epoch + 1}/{epochs}, "
            f"Loss: {total_loss / len(train_loader):.5f}, "
            f"P/L: {pnl:.2f}, "
            f"Best P/L: {best_pnl:.2f}, "
            f"Epoch best P/L: {epoch_best_pnl}, "
            f"seed: {counter}"
        )

        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ P/L ===
        if pnl > best_pnl:
            best_pnl = pnl
            epochs_no_improve = 0
            epoch_best_pnl = epoch + 1
            torch.save(model.state_dict(), model_path)
            print(f"‚úÖ Model saved with P/L: {best_pnl:.2f}")
        else:
            epochs_no_improve += 1

        # === –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ ===
        if epochs_no_improve >= early_stop_epochs:
            print(f"üõë Early stopping at epoch {epoch + 1}")
            break

    # === 7. –ó–ê–ì–†–£–ó–ö–ê –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ò –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ ===
    print("\nüîπ Loading best model for final evaluation...")
    model.load_state_dict(torch.load(model_path))
    model.eval()

    y_preds_final = []
    with torch.no_grad():
        for X_batch, _ in test_loader:
            X_batch = X_batch.to(device)
            y_pred = model(X_batch).squeeze().cpu().numpy()
            y_preds_final.extend(y_pred)

    final_pnl = calculate_pnl(y_preds_final, test_open_prices, test_close_prices)
    print(f"üèÜ Final Test P/L: {final_pnl:.2f}")
