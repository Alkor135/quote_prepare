import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, random_split
from pathlib import Path
from data_read import data_load, balance_classes
import os

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
SEQUENCE_LENGTH = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π –≤ –∏—Å—Ç–æ—Ä–∏–∏
INPUT_SIZE = SEQUENCE_LENGTH  # –ö–æ–ª-–≤–æ —Ñ–∏—á–µ–π (ed_1 - ed_20)
HIDDEN_SIZE = 64  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
NUM_LAYERS = 2  # –ß–∏—Å–ª–æ LSTM-—Å–ª–æ—ë–≤
OUTPUT_SIZE = 2  # –î–≤–∞ –∫–ª–∞—Å—Å–∞: 0 (–ø–∞–¥–µ–Ω–∏–µ) –∏ 1 (—Ä–æ—Å—Ç)
LEARNING_RATE = 0.001
BATCH_SIZE = 32
EPOCHS = 50
MODEL_PATH = Path(r"model/best_lstm_model.pth")

#     return train_loader, val_loader
def prepare_data(df_fut):
    X = df_fut[[f"ed_{i}" for i in range(1, 21)]].values.astype(np.float32)
    y = df_fut["DIRECTION"].values.astype(np.int64)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    train_size = int(0.8 * len(X))
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ train-–¥–∞–Ω–Ω—ã—Ö
    X_train_bal, y_train_bal = balance_classes(X_train, y_train)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    print(f"X_train_bal shape –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {X_train_bal.shape}")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å (samples, 20)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    num_samples = X_train_bal.shape[0]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    X_train_bal = torch.tensor(X_train_bal).reshape(num_samples, 1, INPUT_SIZE)  # (samples, 1, 20)
    y_train_bal = torch.tensor(y_train_bal)

    X_val = torch.tensor(X_val).reshape(-1, 1, INPUT_SIZE)  # (samples, 1, 20)
    y_val = torch.tensor(y_val)

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = TensorDataset(X_train_bal, y_train_bal)
    val_dataset = TensorDataset(X_val, y_val)

    # DataLoader'—ã
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    return train_loader, val_loader

# === 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
        output = self.fc(last_hidden)
        return output

# === 3. –§—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ===
def train_model(train_loader, val_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    best_val_acc = 0.0

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0
        correct, total = 0, 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y_batch).sum().item()
            total += y_batch.size(0)

        train_acc = correct / total
        val_acc = evaluate_model(model, val_loader, device)

        print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), MODEL_PATH)
            print("üî• Model saved!")

# === 4. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ===
def evaluate_model(model, val_loader, device):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y_batch).sum().item()
            total += y_batch.size(0)

    return correct / total

# === 5. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è ===
if __name__ == "__main__":
    script_dir = Path(__file__).parent
    os.chdir(script_dir)

    db_path = Path(r'C:\Users\Alkor\gd\data_quote_db\RTS_futures_options_day_2014.db')
    df_fut = data_load(db_path, '2014-01-01', '2024-01-01')

    train_loader, val_loader = prepare_data(df_fut)
    train_model(train_loader, val_loader)
