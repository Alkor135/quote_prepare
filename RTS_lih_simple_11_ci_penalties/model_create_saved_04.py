import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import random
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
import os
from data_read import data_load, balance_classes
import shutil
import sys
sys.dont_write_bytecode = True

# === –°–û–ó–î–ê–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (LSTM) ===
class CandleLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(CandleLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])
        return self.sigmoid(x)

class CandlestickDataset(Dataset):
    def __init__(self, X, y, bodies, avg_bodies):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.float32)
        self.bodies = torch.tensor(bodies, dtype=torch.float32)
        self.avg_bodies = torch.tensor(avg_bodies, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx], self.bodies[idx], self.avg_bodies[idx]

# === –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ —Å–≤–µ—á–∏ ===
# class CustomLoss(nn.Module):
#     def __init__(self):
#         super().__init__()

#     def forward(self, y_pred, y_true, candle_bodies, avg_body_20):
#         y_pred_classes = (y_pred > 0.5).float()
#         correct = (y_pred_classes == y_true).float()
#         large_body = (candle_bodies > avg_body_20).float()

#         weights = correct * (1 + large_body) + (1 - correct) * (1 + large_body)
#         weights = torch.clamp(weights, min=0.1)  

#         loss = nn.BCELoss(reduction="none")(y_pred, y_true)
#         weighted_loss = loss * weights

#         # print(f"Mean loss: {weighted_loss.mean().item()}")  # –î–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∑–Ω–∞—á–µ–Ω–∏–π
#         return weighted_loss.mean()
class CustomLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, y_pred, y_true, candle_bodies, avg_body_10):
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –∫–ª–∞—Å—Å—ã (0 –∏–ª–∏ 1)
        y_pred_classes = (y_pred > 0.5).float()

        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ (1) –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π (0)
        correct = (y_pred_classes == y_true).float()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–ª–æ —Å–≤–µ—á–∏ –±–æ–ª—å—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
        large_body = (candle_bodies > avg_body_10).float()

        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã:
        # - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Å –±–æ–ª—å—à–∏–º —Ç–µ–ª–æ–º: +2
        # - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Å –º–∞–ª—ã–º —Ç–µ–ª–æ–º: +1
        # - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Å –º–∞–ª—ã–º —Ç–µ–ª–æ–º: +1
        # - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Å –±–æ–ª—å—à–∏–º —Ç–µ–ª–æ–º: +2
        weights = correct * (1 + large_body) + (1 - correct) * (1 + large_body)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        weights = torch.clamp(weights, min=0.1)

        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–∑–æ–≤—É—é –±–∏–Ω–∞—Ä–Ω—É—é –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—é
        loss = nn.BCELoss(reduction="none")(y_pred, y_true)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        weighted_loss = loss * weights

        return weighted_loss.mean()

# === –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á–µ—Ç–∞ P/L ===
def calculate_pnl(y_preds, open_prices, close_prices):
    pnl = 0
    for i in range(len(y_preds)):
        difference = abs(close_prices[i] - open_prices[i])  # –†–∞–∑–º–µ—Ä —Å–≤–µ—á–∏

        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
        weight_correct = 2  # –£—Å–∏–ª–µ–Ω–∏–µ –ø–æ–æ—â—Ä–µ–Ω–∏—è –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑
        weight_incorrect = 2  # –£—Å–∏–ª–µ–Ω–∏–µ —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑

        if y_preds[i] > 0.5:  # –ü—Ä–æ–≥–Ω–æ–∑ —Ä–æ—Å—Ç–∞
            if close_prices[i] > open_prices[i]:  # –†–µ–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç
                pnl += weight_correct * difference  # –£—Å–∏–ª–µ–Ω–Ω–æ–µ –ø–æ–æ—â—Ä–µ–Ω–∏–µ
            else:  # –†–µ–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
                pnl -= weight_incorrect * difference  # –£—Å–∏–ª–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ
        else:  # –ü—Ä–æ–≥–Ω–æ–∑ –ø–∞–¥–µ–Ω–∏—è
            if close_prices[i] < open_prices[i]:  # –†–µ–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
                pnl += weight_correct * difference  # –£—Å–∏–ª–µ–Ω–Ω–æ–µ –ø–æ–æ—â—Ä–µ–Ω–∏–µ
            else:  # –†–µ–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç
                pnl -= weight_incorrect * difference  # –£—Å–∏–ª–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ
    return pnl

# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ===
script_dir = Path(__file__).parent
os.chdir(script_dir)

db_path = Path(r'C:\Users\Alkor\gd\data_quote_db\RTS_day_2014.db')
df = data_load(db_path, '2014-01-01', '2024-01-01')

for counter in range(1, 101):
    shutil.rmtree('__pycache__', ignore_errors=True)

    np.random.seed(counter)
    random.seed(counter)
    torch.manual_seed(counter)

    df_fut = df.copy()

    X = df_fut[[f'CI_{i}' for i in range(1, 21)]].values
    y = df_fut['DIRECTION']
    bodies = df_fut['BODY']
    avg_bodies = df_fut['BODY_AVG']

    X, y, bodies, avg_bodies = map(np.array, [X, y, bodies, avg_bodies])

    split = int(0.85 * len(X))
    X_train, y_train = X[:split], y[:split]
    X_test, y_test = X[split:], y[split:]
    bodies_train, bodies_test = bodies[:split], bodies[split:]
    avg_bodies_train, avg_bodies_test = avg_bodies[:split], avg_bodies[split:]

    X_train, y_train, bodies_train, avg_bodies_train = balance_classes(
        X_train, y_train, bodies_train, avg_bodies_train
        )

    train_dataset = CandlestickDataset(X_train, y_train, bodies_train, avg_bodies_train)
    test_dataset = CandlestickDataset(X_test, y_test, bodies_test, avg_bodies_test)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = CandleLSTM(vocab_size=27, embedding_dim=32, hidden_dim=32, output_dim=1).to(device)
    criterion = CustomLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0005)

    best_pnl = float('-inf')
    epoch_best_pnl = 0
    model_path = Path(fr"model_04\best_model_{counter}.pth")
    early_stop_epochs = 200
    epochs_no_improve = 0

    epochs = 2000
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch, bodies_batch, avg_bodies_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            bodies_batch, avg_bodies_batch = bodies_batch.to(device), avg_bodies_batch.to(device)

            optimizer.zero_grad()
            y_pred = model(X_batch).squeeze()

            # # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–µ—Ä–µ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –ø–æ—Ç–µ—Ä—å
            # print(f"Epoch {epoch + 1} - Sample y_pred (before loss): {y_pred[:5].detach().cpu().numpy()}")
            # print(f"Epoch {epoch + 1} - Sample y_true: {y_batch[:5].cpu().numpy()}")

            loss = criterion(y_pred, y_batch, bodies_batch, avg_bodies_batch)

            # # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è loss
            # print(f"Epoch {epoch + 1} - Loss: {loss.item()}")

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        model.eval()
        y_preds = []
        
        with torch.no_grad():
            for X_batch, _, _, _ in test_loader:
                X_batch = X_batch.to(device)
                y_pred = model(X_batch).squeeze().cpu().numpy()
                y_preds.extend(y_pred)

        test_open_prices = df_fut['OPEN'].iloc[split:].values
        test_close_prices = df_fut['CLOSE'].iloc[split:].values

        # # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π y_preds –ø–µ—Ä–µ–¥ —Ä–∞—Å—á–µ—Ç–æ–º P/L
        # print(f"Epoch {epoch + 1} - Sample y_preds: {y_preds[:5]}")

        pnl = calculate_pnl(y_preds, test_open_prices, test_close_prices)

        print(
            f"Epoch {epoch + 1}/{epochs}, "
            f"Loss: {total_loss / len(train_loader):.5f}, "
            f"P/L: {pnl:.2f}, "
            f"Best P/L: {best_pnl:.2f}, "
            f"Epoch best P/L: {epoch_best_pnl}, "
            f"seed: {counter}"
        )

        if pnl > best_pnl:
            print(f"‚úÖ New Best P/L found: {pnl:.2f} (Previous: {best_pnl:.2f})")
            best_pnl = pnl
            epochs_no_improve = 0
            epoch_best_pnl = epoch + 1
            torch.save(model.state_dict(), model_path)
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= early_stop_epochs:
            print(f"üõë Early stopping at epoch {epoch + 1}")
            break

    print("\nüîπ Loading best model for final evaluation...")
    model.load_state_dict(torch.load(model_path))
    model.eval()

    y_preds_final = []
    with torch.no_grad():
        for X_batch, _, _, _ in test_loader:
            X_batch = X_batch.to(device)
            y_pred = model(X_batch).squeeze().cpu().numpy()
            y_preds_final.extend(y_pred)

    final_pnl = calculate_pnl(y_preds_final, test_open_prices, test_close_prices)
    print(f"üèÜ Final Test P/L: {final_pnl:.2f}")
